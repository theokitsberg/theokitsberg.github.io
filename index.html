<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theo Kitsberg</title>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css?v=2">
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
                <div class="text-content">
                    <h1>Theo Kitsberg</h1>
                    <p class="intro">
                        I'm currently studying Philosophy and some Psychology at the
                        <a href="https://www.christs.cam.ac.uk/" target="_blank" rel="noopener noreferrer">University of Cambridge</a>.
                        This summer, I'll be at
                        <a href="https://www.ucl.ac.uk/gatsby/gatsby-computational-neuroscience-unit" target="_blank" rel="noopener noreferrer">UCL's Gatsby Unit</a>
                        for an intensive program in mathematics for machine learning.
                        Following that, I'll be working with
                        <a href="https://nocklab.fas.harvard.edu/people/daniel-low" target="_blank" rel="noopener noreferrer">Daniel Low</a>
                        at the intersection of causal inference, NLP, and crisis response and exploring optimal natural language outputs for people in distress, whilst evaluating how generative AI performs in these contexts. At the moment, I am spending a lot of time with the amazing <a href="https://www.media.mit.edu/groups/fluid-interfaces/overview/" target="_blank" rel="noopener noreferrer">Fluid Interfaces group</a> at <a href="https://www.media.mit.edu/" target="_blank" rel="noopener noreferrer">MIT's Media Lab</a>.
                    </p>
                    <p class="contact-info">
                        If you want more information about anything here, a chat, or my CV, please
                        <a href="mailto:tck32@cam.ac.uk">email me</a>. Feel free to check out my <a href="https://github.com/theokitsberg" target="_blank" rel="noopener noreferrer">github</a>.
                    </p>
                </div>
                <div class="profile-image">
                    <img src="images/profile.jpg" alt="Profile photo" class="profile-photo">
                </div>
            </div>
        </header>

        <main>
            <section id="interests">
                <h2>Interests</h2>
                <p>
                    As LLMs are increasingly deployed in high-risk and high-impact settings, I'm interested in how we can ensure they behave in a manner which is not only consistent but consistently good, all whilst trying to solve that age-old question of how exactly we ought to understand "good". I am also curious about if we should be willing to give models a pass for less consistent behaviour and, if so, when. For better or for worse, these questions have become time-sensitive.
                </p>
                <p>
                    Currently, I am especially interested in situations which affect mental-wellbeing. Arguably virtually every interaction has the potential to, even if only indirectly, so I spend a decent amount of time thinking about quotidian interactions along with the more crisis-laden ones.

                    More tangentially, I'm also interested in how language models interact with each other and in understanding authenticity (of relationships, work, achievement, identity etc.) in the context of AI developments. 
                </p
            </section>

            <section id="projects">
                <h2>Relevant Work</h2>
                
                <h3>Personal Projects</h3>
                <div class="project">
                    <h4><a href="fourthwallrpo.pdf" target="_blank" rel="noopener noreferrer">dAIvd: simulating my PhD friend's approach to philosophy with Recursive Prompt Optimisation (RPO)</a></h4>
                    <p>
                        Built a Recursive Prompt Optimisation (RPO) pipeline that improves Claude Sonnet 3.7's ability to roleplay as my friend David. At each iteration Claude attempts to answer a philosophical question, and this is checked by a Claude discriminator agent against David's old work. If the discriminator can correctly determine the imposter, it explains why and a prompt-generator Claude uses that feedback to rewrite the system-prompt; the loop then repeats until the discriminator fails three times in a row. <strong>Note: there are possibly some interesting applications of RPO to outer-alignment work...</strong>
                <div class="project">
                    <h4><a href="moralpsychstability.pdf" target="_blank" rel="noopener noreferrer">Measuring the Stability of LLM Moral Psychology</a></h4>
                    <p>
                        Explored the moral stability of large language models by investigating what I call moral drift. I focused on three levels of evaluation: first, how consistent models are in their moral decision-making when responding to ethical dilemmas under standard versus high-pressure conditions; second, how stable their moral judgments are when those judgments are subjected to direct philosophical critique or pushback; and third, how well a model's own decisions align with its evaluative reasoningâ€”in other words, whether the models judge actions (their own or others') in a way that's coherent with how they make decisions themselves. The goal wasn't to decide if a model is moral, but to understand how its moral reasoning holds together when challenged or strained.<strong>Working on some visualisations - check back soon, but feel free to checkout my code (all 2800 lines of it) and my data on <a href="https://github.com/theokitsberg" target="_blank" rel="noopener noreferrer">github</a>!</strong>
                    </p>
                </div>

                <div class="project">
                    <h4><a href="listeningllama.pdf" target="_blank" rel="noopener noreferrer">Listening Llama</a></h4>
                    <p>
                        Fine-tuned Llama 7B to reliably respond to emotionally charged disclosures in the manner of a helpline volunteer. Generated 1047 synthetic datapoints through review and improvement of batched GPT generations then scaled that dataset to 10,000 via a haiku API creative rephrasing script. Cleaned the data then fine-tuned llama with instruction-tuning and DPO-tuning. <strong>Constitutional AI and evals coming soon.</strong>
                    </p>
                </div>
                
                <div class="project">
                    <h4><a href="Lawtomate.pdf" target="_blank" rel="noopener noreferrer">Lawtomate: Towards a monolithic, non-iterative prompt for Cambridge Law exams</a></h4>
                    <p>
                        Showing (contrary to popular perception in Cambridge) that students can use LLMs to produce high-quality answers to Cambridge law exams with minimal effort or legal understanding. Moved from API automation to a web UI process and (soon) to a single, monolithic prompt, pushing the technical barrier to entry to the floor. Want to expose equity issues in open-book, online exams.
                    </p>
                </div>

                <div class="project">
                    <h4><a href="Midas Models.pdf" target="_blank" rel="noopener noreferrer">Some Ongoing, Incrementally Updated Thoughts on LLMs and Mental Health</a></h4>
                    <p>
                        My attempt to unravel what we should think and feel about uses of LLMs in behavioural care contexts, such as therapy or crisis care. <strong>Down for now as I clarify some of my thoughts in light of recent research, conversations and experiences.</strong>
                    </p>
                </div>

                <h3>Experiences in Industry and Academia</h3>
                <div class="project">
                    <p>
                        Most recently, I have spent time at the <a href="https://cambridge-afar.github.io/" target="_blank" rel="noopener noreferrer">Affective Intelligence and Robotics Lab</a>, testing affect integration in <a href="https://arxiv.org/abs/2407.01488" target="_blank" rel="noopener noreferrer">LEXI</a> to reduce benevolent prejudice and deriving RL reward functions for group HRI from social science literature. Before that, I worked at <a href="https://www.cambridgemindtechnologies.com/" target="_blank" rel="noopener noreferrer">Cambridge Mind Technologies</a>, on red-teaming Cami (their LLM for adolescent support) and testing it against competitors. Last summer, at <a href="https://www.lionheart.vc/" target="_blank" rel="noopener noreferrer">Lionheart Ventures</a>, I spent time assessing AI investment risks through LLM-assisted scenario discovery, alongside more traditional sourcing and diligence work. Before Cambridge, I had my first experience in industry at <a href="https://www.deepchecks.com/" target="_blank" rel="noopener noreferrer">Deepchecks</a>, where I worked on LLM grounding and query avoidance through red-teaming.
                    </p>
                </div>
            </section>

            <section id="hobbies">
                <h2>Hobbies</h2>
                <p>
                    I like to run, and my favourite distance sits somewhere between 10 and 20 km. I'm still undecided on whether I prefer the scenic river route in Cambridge or my equally scenic route back home, which, incidentally, runs alongside a main road. Occasionally, I lift weights, and I once tried Brazilian Jiu Jitsu, where I discovered I'm fantastic at being thrown to the floor and strangled. When I was younger, I used to buy and sell used and collectible books to financially support my reading habits. I still dabble if I see something really special, but I prefer libraries to eBay these days.
                </p>
            </section>
        </main>
    </div>
</body>
</html> 
